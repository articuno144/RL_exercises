{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize variables\n",
    "n_actions = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-01 12:34:02,227] Making new env: Pong-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pong-v0\")\n",
    "observation = env.reset()\n",
    "prev_x = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro(I):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return I.astype(np.float).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 80, 80, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 78, 78, 16)   160         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 39, 39, 16)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 37, 37, 16)   2320        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 18, 18, 16)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 16, 16, 16)   2320        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 16)     0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1024)         0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1026)         0           input_2[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           65728       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           2080        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            33          dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 72,641\n",
      "Trainable params: 72,641\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img = keras.layers.Input(shape=(80,80,1))\n",
    "conv1 = Conv2D(16, (3, 3), activation='relu')(img)\n",
    "maxpool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(16, (3, 3), activation='relu')(maxpool1)\n",
    "maxpool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "conv3 = Conv2D(16, (3, 3), activation='relu')(maxpool2)\n",
    "maxpool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "flatten = Flatten()(maxpool3)\n",
    "input2 = keras.layers.Input(shape=(n_actions,))\n",
    "concat = keras.layers.Concatenate(axis=-1)([input2,flatten])\n",
    "dense1 = Dense(64, activation='relu')(concat)\n",
    "dense2 = Dense(32, activation='relu')(dense1)\n",
    "out = keras.layers.Dense(1, activation='relu')(dense2)\n",
    "model = keras.models.Model(inputs=[img, input2], outputs=out)\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = np.zeros([1,80,80,1])\n",
    "a = np.zeros([1,2])\n",
    "model.predict([s,a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q(s,n_a):\n",
    "    \"\"\"takes the state, and the number of actions. Returns a numpy array of estimates of Q(s,a)\"\"\"\n",
    "    q = np.zeros(n_a)\n",
    "    for i in range(n_a):\n",
    "        a = np.zeros(n_a)\n",
    "        a[i] = 1\n",
    "        q[i] = model.predict([s,a])\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(Q):\n",
    "    \"\"\"pick action based on Q\"\"\"\n",
    "    \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "states,actions,drs = [],[],[]\n",
    "episode_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    env.render()\n",
    "    # 1 iter per frame\n",
    "    cur_s = prepro(observation)\n",
    "    s = cur_s - prev_s if prev_s is not None else np.zeros(D)\n",
    "    prev_s = cur_s\n",
    "    \n",
    "    # choose action, apply, get measurements\n",
    "    Q = (s,n_actions)\n",
    "    a = act(Q)\n",
    "    observation, reward, done, info = env.step(action+2)\n",
    "    states.append(s)\n",
    "    actions.append(a)\n",
    "    rewards.append(reward)\n",
    "    if reward == 1: print(\"!!!!!\")\n",
    "    \n",
    "    # when finished, calculate rewards, train Q against these rewards\n",
    "    if done:\n",
    "        episode_number += 1\n",
    "        eps = np.vstack(states)\n",
    "        epa = np.vstack(actions)\n",
    "        epr = discount_rewards(np.vstack(rewards))\n",
    "        print(np.sum(rewards))\n",
    "        states,actions,drs = [],[],[]\n",
    "        if episode_number%32==0:\n",
    "            model.fit(x=[eps,eps],y=epr,epochs=8,verbose=0)\n",
    "        observation = env.reset()\n",
    "        prev_s = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method step in module gym.core:\n",
      "\n",
      "step(action) method of gym.wrappers.time_limit.TimeLimit instance\n",
      "    Run one timestep of the environment's dynamics. When end of\n",
      "    episode is reached, you are responsible for calling `reset()`\n",
      "    to reset this environment's state.\n",
      "    \n",
      "    Accepts an action and returns a tuple (observation, reward, done, info).\n",
      "    \n",
      "    Args:\n",
      "        action (object): an action provided by the environment\n",
      "    \n",
      "    Returns:\n",
      "        observation (object): agent's observation of the current environment\n",
      "        reward (float) : amount of reward returned after previous action\n",
      "        done (boolean): whether the episode has ended, in which case further step() calls will return undefined results\n",
      "        info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(env.step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
